17/07/03 18:42:16 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 18:42:16 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 18:42:16 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 18:42:16 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 18:42:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 18:42:19 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 18:42:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 18:42:22 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 18:42:22 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 18:42:22 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 18:42:22 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 18:42:32 INFO StreamParseJson: no records
17/07/03 18:43:00 INFO StreamParseJson: no records
17/07/03 18:43:30 INFO StreamParseJson: no records
17/07/03 18:44:00 INFO StreamParseJson: has records
17/07/03 18:44:08 INFO ParseReport: 将解析完的报告存入ES中
17/07/03 18:44:12 INFO ParseReport: 将labels保存到Hive中
17/07/03 18:44:30 INFO StreamParseJson: no records
17/07/03 18:47:36 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 18:47:36 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 18:47:36 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 18:47:36 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 18:47:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 18:47:40 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 18:47:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 18:47:42 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 18:47:42 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 18:47:42 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 18:47:42 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 18:48:01 INFO StreamParseJson: has records
17/07/03 18:48:08 INFO ParseReport: 将解析完的报告存入ES中
17/07/03 18:48:12 INFO ParseReport: 将labels保存到Hive中
17/07/03 18:48:19 INFO ParseReport: 将index保存到Hive
17/07/03 18:48:21 INFO ParseReport: 将summary保存到Hive中
17/07/03 18:48:22 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.sql.AnalysisException: cannot resolve '(`reportContent`.`generalSummarys` = CAST('' AS DOUBLE))' due to data type mismatch: differing types in '(`reportContent`.`generalSummarys` = CAST('' AS DOUBLE))' (array<string> and double).;;
'Filter (reportContent#15.generalSummarys = cast( as double))
+- Project [obj#1.healthReportId AS healthReportId#8L, obj#1.userId AS userId#9, obj#1.idCardNoMd5 AS idCardNoMd5#10, obj#1.birthday AS birthday#11, obj#1.sex AS sex#12, obj#1.checkUnitCode AS checkUnitCode#13, obj#1.checkUnitName AS checkUnitName#14, obj#1.reportContent AS reportContent#15, obj#1.checkDate AS checkDate#16, obj#1.lastUpdateTime AS lastUpdateTime#17]
   +- Filter ((upper(eventType#0) = INSERT) && (upper(table#2) = REPORT))
      +- LogicalRDD [eventType#0, obj#1, table#2]

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:58)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2850)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1282)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.parseSuggestsProcess(ParseReport.scala:471)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.suggestsToHive(ParseReport.scala:363)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:461)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 18:48:30 INFO StreamParseJson: no records
17/07/03 18:54:45 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 18:54:45 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 18:54:45 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 18:54:45 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 18:54:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 18:54:48 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 18:54:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 18:54:51 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 18:54:51 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 18:54:51 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 18:54:51 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 18:55:00 INFO StreamParseJson: no records
17/07/03 18:55:30 INFO StreamParseJson: no records
17/07/03 18:56:00 INFO StreamParseJson: has records
17/07/03 18:56:08 INFO ParseReport: 将解析完的报告存入ES中
17/07/03 18:56:12 INFO ParseReport: 将labels保存到Hive中
17/07/03 18:56:19 INFO ParseReport: 将index保存到Hive
17/07/03 18:56:21 INFO ParseReport: 将summary保存到Hive中
17/07/03 18:56:22 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.sql.AnalysisException: cannot resolve '(NOT `reportContent`.`generalSummarys`)' due to data type mismatch: argument 1 requires boolean type, however, '`reportContent`.`generalSummarys`' is of array<struct<fw:string,result:string,reviewAdvice:string,summaryAdvice:string,summaryDescription:string,summaryMedicalExplanation:string,summaryName:string,summaryReasonResult:string>> type.;;
'Filter (NOT reportContent#15.generalSummarys = )
+- Project [obj#1.healthReportId AS healthReportId#8L, obj#1.userId AS userId#9, obj#1.idCardNoMd5 AS idCardNoMd5#10, obj#1.birthday AS birthday#11, obj#1.sex AS sex#12, obj#1.checkUnitCode AS checkUnitCode#13, obj#1.checkUnitName AS checkUnitName#14, obj#1.reportContent AS reportContent#15, obj#1.checkDate AS checkDate#16, obj#1.lastUpdateTime AS lastUpdateTime#17]
   +- Filter ((upper(eventType#0) = INSERT) && (upper(table#2) = REPORT))
      +- LogicalRDD [eventType#0, obj#1, table#2]

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:58)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2850)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1282)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.parseSuggestsProcess(ParseReport.scala:470)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.suggestsToHive(ParseReport.scala:363)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:461)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 18:56:30 INFO StreamParseJson: no records
17/07/03 18:57:00 INFO StreamParseJson: no records
17/07/03 18:57:30 INFO StreamParseJson: no records
17/07/03 18:58:00 INFO StreamParseJson: no records
17/07/03 18:58:30 INFO StreamParseJson: has records
17/07/03 18:58:33 INFO ParseReport: 将解析完的报告存入ES中
17/07/03 18:58:33 WARN BlockManager: Block rdd_91_0 already exists on this machine; not re-adding it
17/07/03 18:58:34 INFO ParseReport: 将labels保存到Hive中
17/07/03 18:58:36 WARN Shell: Interrupted while joining on: Thread[Thread-53,5,main]
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1252)
	at java.lang.Thread.join(Thread.java:1326)
	at org.apache.hadoop.util.Shell.joinThread(Shell.java:589)
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:540)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:815)
	at org.apache.hadoop.util.Shell.execCommand(Shell.java:798)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:728)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:486)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:527)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:504)
	at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:305)
	at org.apache.hadoop.hive.ql.session.SessionState.createPath(SessionState.java:732)
	at org.apache.hadoop.hive.ql.session.SessionState.createSessionDirs(SessionState.java:675)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:563)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:526)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.createPartitionIfNotExists(HiveEndPoint.java:455)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:318)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:278)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:215)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:192)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:105)
	at com.haozhuo.bigdata.dataetl.hive.HiveCatalog.init(HiveCatalog.java:34)
	at com.haozhuo.bigdata.dataetl.hive.HiveCatalog.save(HiveCatalog.java:71)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.labelsToHive(ParseReport.scala:447)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:458)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 18:58:36 ERROR HiveCatalog: Error
java.lang.IllegalStateException: Shutdown in progress, cannot add a deleteOnExit
	at org.apache.hive.common.util.ShutdownHookManager.deleteOnExit(ShutdownHookManager.java:225)
	at org.apache.hadoop.hive.common.FileUtils.createTempFile(FileUtils.java:799)
	at org.apache.hadoop.hive.ql.session.SessionState.createTempFile(SessionState.java:938)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:568)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:526)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.createPartitionIfNotExists(HiveEndPoint.java:455)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:318)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint$ConnectionImpl.<init>(HiveEndPoint.java:278)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnectionImpl(HiveEndPoint.java:215)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:192)
	at org.apache.hive.hcatalog.streaming.HiveEndPoint.newConnection(HiveEndPoint.java:105)
	at com.haozhuo.bigdata.dataetl.hive.HiveCatalog.init(HiveCatalog.java:34)
	at com.haozhuo.bigdata.dataetl.hive.HiveCatalog.save(HiveCatalog.java:71)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.labelsToHive(ParseReport.scala:447)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:458)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 18:58:36 ERROR HiveCatalog: Error
java.lang.NullPointerException
	at com.haozhuo.bigdata.dataetl.hive.HiveCatalog.save(HiveCatalog.java:75)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.labelsToHive(ParseReport.scala:447)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:458)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 18:58:37 INFO ParseReport: 将index保存到Hive
17/07/03 18:59:06 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 18:59:06 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 18:59:06 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 18:59:06 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 18:59:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 18:59:11 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 18:59:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 18:59:16 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 18:59:16 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 18:59:16 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 18:59:16 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 18:59:32 INFO StreamParseJson: has records
17/07/03 18:59:46 INFO ParseReport: 将解析完的报告存入ES中
17/07/03 18:59:50 INFO ParseReport: 将labels保存到Hive中
17/07/03 18:59:56 INFO ParseReport: 将index保存到Hive
17/07/03 18:59:57 INFO ParseReport: 将summary保存到Hive中
17/07/03 18:59:59 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.sql.AnalysisException: cannot resolve '(`reportContent`.`generalSummarys` = CAST('' AS DOUBLE))' due to data type mismatch: differing types in '(`reportContent`.`generalSummarys` = CAST('' AS DOUBLE))' (array<struct<fw:string,result:string,reviewAdvice:string,summaryAdvice:string,summaryDescription:string,summaryMedicalExplanation:string,summaryName:string,summaryReasonResult:string>> and double).;;
'Filter NOT (reportContent#15.generalSummarys = cast( as double))
+- Project [obj#1.healthReportId AS healthReportId#8L, obj#1.userId AS userId#9, obj#1.idCardNoMd5 AS idCardNoMd5#10, obj#1.birthday AS birthday#11, obj#1.sex AS sex#12, obj#1.checkUnitCode AS checkUnitCode#13, obj#1.checkUnitName AS checkUnitName#14, obj#1.reportContent AS reportContent#15, obj#1.checkDate AS checkDate#16, obj#1.lastUpdateTime AS lastUpdateTime#17]
   +- Filter ((upper(eventType#0) = INSERT) && (upper(table#2) = REPORT))
      +- LogicalRDD [eventType#0, obj#1, table#2]

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:58)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2850)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1282)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.parseSuggestsProcess(ParseReport.scala:470)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.suggestsToHive(ParseReport.scala:363)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:461)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 19:00:00 INFO StreamParseJson: no records
17/07/03 19:00:30 INFO StreamParseJson: no records
17/07/03 19:04:27 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 19:04:27 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 19:04:27 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 19:04:27 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 19:04:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 19:04:31 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 19:04:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 19:04:34 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 19:04:34 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 19:04:34 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 19:04:34 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 19:05:08 INFO StreamParseJson: has records
17/07/03 19:05:30 INFO StreamParseJson: no records
17/07/03 19:06:00 INFO StreamParseJson: no records
17/07/03 19:06:30 INFO StreamParseJson: no records
17/07/03 19:07:12 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 19:07:12 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 19:07:12 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 19:07:12 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 19:07:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 19:07:18 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 19:07:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 19:07:28 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 19:07:28 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 19:07:28 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 19:07:28 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 19:07:33 INFO StreamParseJson: has records
17/07/03 19:07:44 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.sql.AnalysisException: cannot resolve 'isnan(`reportContent`.`generalSummarys`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`reportContent`.`generalSummarys`' is of array<struct<fw:string,result:string,reviewAdvice:string,summaryAdvice:string,summaryDescription:string,summaryMedicalExplanation:string,summaryName:string,summaryReasonResult:string>> type.;;
'Filter NOT isnan(reportContent#15.generalSummarys)
+- Project [obj#1.healthReportId AS healthReportId#8L, obj#1.userId AS userId#9, obj#1.idCardNoMd5 AS idCardNoMd5#10, obj#1.birthday AS birthday#11, obj#1.sex AS sex#12, obj#1.checkUnitCode AS checkUnitCode#13, obj#1.checkUnitName AS checkUnitName#14, obj#1.reportContent AS reportContent#15, obj#1.checkDate AS checkDate#16, obj#1.lastUpdateTime AS lastUpdateTime#17]
   +- Filter ((upper(eventType#0) = INSERT) && (upper(table#2) = REPORT))
      +- LogicalRDD [eventType#0, obj#1, table#2]

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:91)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:290)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:305)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:287)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:266)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:276)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:255)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:83)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:128)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:161)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:167)
	at org.apache.spark.sql.Dataset$.apply(Dataset.scala:58)
	at org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:2850)
	at org.apache.spark.sql.Dataset.filter(Dataset.scala:1282)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.parseSuggestsProcess(ParseReport.scala:470)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.suggestsToHive(ParseReport.scala:363)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:461)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 19:08:00 INFO StreamParseJson: no records
17/07/03 19:08:30 INFO StreamParseJson: no records
17/07/03 19:09:00 INFO StreamParseJson: no records
17/07/03 19:09:30 INFO StreamParseJson: no records
17/07/03 19:10:04 INFO StreamParseJson: no records
17/07/03 19:10:30 WARN ConsumerCoordinator: Auto-commit of offsets {dev-syn-table-reportcontent-0=OffsetAndMetadata{offset=21677, metadata=''}, dev-syn-table-reportcontent-1=OffsetAndMetadata{offset=21691, metadata=''}} failed for group dataetl-labelgen: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
17/07/03 19:10:30 WARN ConsumerCoordinator: Auto-commit of offsets {dev-syn-table-reportcontent-0=OffsetAndMetadata{offset=21677, metadata=''}, dev-syn-table-reportcontent-1=OffsetAndMetadata{offset=21691, metadata=''}} failed for group dataetl-labelgen: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
17/07/03 19:10:30 INFO StreamParseJson: no records
17/07/03 19:11:01 WARN ConsumerCoordinator: Auto-commit of offsets {dev-syn-table-reportcontent-0=OffsetAndMetadata{offset=21677, metadata=''}, dev-syn-table-reportcontent-1=OffsetAndMetadata{offset=21691, metadata=''}} failed for group dataetl-labelgen: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
17/07/03 19:11:02 INFO StreamParseJson: no records
17/07/03 19:11:30 INFO StreamParseJson: no records
17/07/03 19:23:45 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 19:23:45 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 19:23:45 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 19:23:45 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 19:23:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 19:23:50 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 19:23:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 19:23:59 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 19:23:59 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 19:23:59 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 19:23:59 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 19:24:30 INFO StreamParseJson: has records
17/07/03 19:25:00 INFO StreamParseJson: no records
17/07/03 19:25:30 INFO StreamParseJson: no records
17/07/03 19:26:00 INFO StreamParseJson: no records
17/07/03 19:26:30 INFO StreamParseJson: has records
17/07/03 19:27:00 INFO StreamParseJson: no records
17/07/03 19:27:30 INFO StreamParseJson: no records
17/07/03 19:28:00 INFO StreamParseJson: no records
17/07/03 19:28:30 INFO StreamParseJson: has records
17/07/03 19:29:00 INFO StreamParseJson: no records
17/07/03 19:29:30 INFO StreamParseJson: no records
17/07/03 19:30:00 INFO StreamParseJson: no records
17/07/03 19:30:30 INFO StreamParseJson: no records
17/07/03 19:31:21 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 19:31:21 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 19:31:21 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 19:31:21 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 19:31:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 19:31:25 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 19:31:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 19:31:28 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 19:31:28 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 19:31:28 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 19:31:28 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 19:31:31 INFO StreamParseJson: has records
17/07/03 19:32:00 INFO StreamParseJson: no records
17/07/03 19:32:30 INFO StreamParseJson: no records
17/07/03 19:33:00 INFO StreamParseJson: no records
17/07/03 19:33:22 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 19:33:22 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 19:33:22 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 19:33:22 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 19:33:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 19:33:27 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 19:33:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 19:33:37 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 19:33:37 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 19:33:37 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 19:33:37 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 19:34:01 INFO StreamParseJson: has records
17/07/03 19:34:08 ERROR StreamParseJson: 解析报告出错了
java.lang.ArrayStoreException: scala.collection.mutable.WrappedArray$ofRef
	at scala.collection.mutable.ArrayBuilder$ofRef.$plus$eq(ArrayBuilder.scala:87)
	at scala.collection.mutable.ArrayBuilder$ofRef.$plus$eq(ArrayBuilder.scala:56)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2386)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2385)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2390)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2390)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2801)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2390)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2366)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.parseSuggestsProcess(ParseReport.scala:470)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.suggestsToHive(ParseReport.scala:363)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHive(ParseReport.scala:461)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:43)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:30)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/03 19:34:30 INFO StreamParseJson: no records
17/07/03 19:35:26 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 19:35:26 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 19:35:26 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 19:35:26 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 19:35:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 19:35:31 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 19:35:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 19:35:41 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 19:35:41 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 19:35:41 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 19:35:41 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 19:36:01 INFO StreamParseJson: has records
17/07/03 19:36:30 INFO StreamParseJson: no records
17/07/03 19:37:00 INFO StreamParseJson: no records
17/07/03 19:37:30 INFO StreamParseJson: no records
17/07/03 19:38:00 INFO StreamParseJson: no records
17/07/03 19:38:30 INFO StreamParseJson: no records
17/07/03 19:39:00 INFO StreamParseJson: no records
17/07/03 19:39:30 INFO StreamParseJson: no records
17/07/03 19:40:00 INFO StreamParseJson: no records
17/07/03 19:40:30 INFO StreamParseJson: no records
17/07/03 19:40:51 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/03 19:40:51 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.url
17/07/03 19:40:51 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.user
17/07/03 19:40:51 WARN Props$: config.properties文件中沒有这个属性:mysql.connection.password
17/07/03 19:40:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/03 19:40:56 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/03 19:40:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/03 19:41:01 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/03 19:41:01 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/03 19:41:01 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/03 19:41:01 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/03 19:41:31 INFO StreamParseJson: has records
17/07/03 19:42:00 INFO StreamParseJson: has records
17/07/03 19:42:30 INFO StreamParseJson: has records
17/07/03 19:43:00 INFO StreamParseJson: has records
17/07/03 19:43:30 INFO StreamParseJson: no records
17/07/03 19:44:00 INFO StreamParseJson: no records
17/07/03 19:44:30 INFO StreamParseJson: no records
17/07/03 19:45:00 INFO StreamParseJson: no records
17/07/03 19:45:30 INFO StreamParseJson: no records
17/07/03 19:46:00 INFO StreamParseJson: no records
17/07/03 19:46:30 INFO StreamParseJson: no records
17/07/03 19:47:00 INFO StreamParseJson: no records
17/07/03 19:47:30 INFO StreamParseJson: no records
17/07/03 19:48:00 INFO StreamParseJson: no records
17/07/03 19:48:30 INFO StreamParseJson: no records
17/07/03 19:49:18 INFO StreamParseJson: no records
17/07/03 19:49:30 WARN ConsumerCoordinator: Auto-commit of offsets {dev-syn-table-reportcontent-0=OffsetAndMetadata{offset=21690, metadata=''}, dev-syn-table-reportcontent-1=OffsetAndMetadata{offset=21699, metadata=''}} failed for group dataetl-labelgen: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
17/07/03 19:49:40 INFO StreamParseJson: no records
17/07/03 19:50:00 WARN ConsumerCoordinator: Auto-commit of offsets {dev-syn-table-reportcontent-0=OffsetAndMetadata{offset=21690, metadata=''}, dev-syn-table-reportcontent-1=OffsetAndMetadata{offset=21699, metadata=''}} failed for group dataetl-labelgen: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
17/07/03 19:50:00 INFO StreamParseJson: no records
17/07/03 19:50:30 INFO StreamParseJson: no records
17/07/03 19:51:00 INFO StreamParseJson: no records
17/07/03 19:51:31 INFO StreamParseJson: no records
17/07/03 19:52:00 INFO StreamParseJson: no records
17/07/03 19:52:30 INFO StreamParseJson: no records
17/07/03 19:53:00 INFO StreamParseJson: no records
17/07/03 19:53:30 INFO StreamParseJson: no records
17/07/03 19:54:00 INFO StreamParseJson: no records
17/07/03 19:54:30 INFO StreamParseJson: no records
17/07/03 19:55:00 INFO StreamParseJson: no records
17/07/03 19:55:30 INFO StreamParseJson: no records
17/07/03 19:56:00 INFO StreamParseJson: no records
17/07/03 19:56:30 INFO StreamParseJson: no records
17/07/03 19:57:00 INFO StreamParseJson: no records
17/07/03 19:57:30 INFO StreamParseJson: no records
17/07/03 19:58:00 INFO StreamParseJson: no records
17/07/03 19:58:30 INFO StreamParseJson: no records
17/07/03 19:59:00 INFO StreamParseJson: no records
17/07/03 19:59:30 INFO StreamParseJson: no records
17/07/03 20:00:00 INFO StreamParseJson: no records
17/07/03 20:00:30 INFO StreamParseJson: no records
17/07/03 20:01:00 INFO StreamParseJson: no records
17/07/03 20:01:30 INFO StreamParseJson: no records
17/07/03 20:02:00 INFO StreamParseJson: no records
17/07/03 20:02:30 INFO StreamParseJson: no records
17/07/03 20:03:00 INFO StreamParseJson: no records
17/07/03 20:03:30 INFO StreamParseJson: no records
17/07/03 20:04:00 INFO StreamParseJson: no records
17/07/03 20:04:30 INFO StreamParseJson: no records
17/07/03 20:05:00 INFO StreamParseJson: no records
17/07/03 20:05:30 INFO StreamParseJson: no records
17/07/03 20:06:00 INFO StreamParseJson: no records
17/07/03 20:06:30 INFO StreamParseJson: no records
17/07/03 20:07:00 INFO StreamParseJson: no records
17/07/03 20:07:30 INFO StreamParseJson: no records
17/07/03 20:08:00 INFO StreamParseJson: no records
17/07/03 20:08:30 INFO StreamParseJson: no records
17/07/03 20:09:00 INFO StreamParseJson: no records
17/07/03 20:09:30 INFO StreamParseJson: no records
17/07/03 20:10:00 INFO StreamParseJson: no records
17/07/03 20:10:30 INFO StreamParseJson: no records
17/07/03 20:11:00 INFO StreamParseJson: no records
17/07/03 20:11:30 INFO StreamParseJson: no records
17/07/03 20:12:00 INFO StreamParseJson: no records
17/07/03 20:12:30 INFO StreamParseJson: no records
17/07/03 20:13:00 INFO StreamParseJson: no records
17/07/03 20:13:30 INFO StreamParseJson: no records
17/07/03 20:14:00 INFO StreamParseJson: no records
17/07/03 20:14:30 INFO StreamParseJson: no records
17/07/03 20:15:00 INFO StreamParseJson: no records
17/07/03 20:15:30 INFO StreamParseJson: no records
17/07/03 20:16:00 INFO StreamParseJson: no records
17/07/03 20:16:30 INFO StreamParseJson: no records
17/07/03 20:17:00 INFO StreamParseJson: no records
17/07/03 20:17:30 INFO StreamParseJson: no records
17/07/03 20:18:00 INFO StreamParseJson: no records
17/07/03 20:18:30 INFO StreamParseJson: no records
17/07/03 20:19:00 INFO StreamParseJson: no records
17/07/03 20:19:30 INFO StreamParseJson: no records
17/07/03 20:20:00 INFO StreamParseJson: no records
17/07/03 20:20:30 INFO StreamParseJson: no records
17/07/03 20:21:00 INFO StreamParseJson: no records
17/07/03 20:21:30 INFO StreamParseJson: no records
17/07/03 20:22:00 INFO StreamParseJson: no records
17/07/03 20:22:30 INFO StreamParseJson: no records
17/07/03 20:23:00 INFO StreamParseJson: no records
17/07/03 20:23:30 INFO StreamParseJson: no records
17/07/03 20:24:00 INFO StreamParseJson: no records
17/07/03 20:24:30 INFO StreamParseJson: no records
17/07/03 20:25:06 INFO StreamParseJson: no records
17/07/03 20:25:36 INFO StreamParseJson: no records
17/07/03 20:26:00 INFO StreamParseJson: no records
17/07/03 20:26:30 INFO StreamParseJson: no records
17/07/03 20:27:00 INFO StreamParseJson: no records
17/07/03 20:27:30 INFO StreamParseJson: no records
17/07/03 20:28:00 INFO StreamParseJson: no records
17/07/03 20:28:30 INFO StreamParseJson: no records
17/07/03 20:29:00 INFO StreamParseJson: no records
17/07/03 20:29:30 INFO StreamParseJson: no records
17/07/03 20:30:00 INFO StreamParseJson: no records
17/07/03 20:30:32 INFO StreamParseJson: no records
17/07/03 20:31:00 INFO StreamParseJson: no records
17/07/03 20:31:30 INFO StreamParseJson: no records
17/07/03 20:32:00 INFO StreamParseJson: no records
17/07/03 20:32:30 INFO StreamParseJson: no records
17/07/03 20:33:00 INFO StreamParseJson: no records
17/07/03 20:33:30 INFO StreamParseJson: no records
17/07/03 20:34:00 INFO StreamParseJson: no records
17/07/03 20:34:30 INFO StreamParseJson: no records
17/07/03 20:35:00 INFO StreamParseJson: no records
17/07/03 20:35:30 INFO StreamParseJson: no records
17/07/03 20:36:00 INFO StreamParseJson: no records
17/07/03 20:36:30 INFO StreamParseJson: no records
17/07/03 20:37:00 INFO StreamParseJson: no records
17/07/03 20:37:30 INFO StreamParseJson: no records
17/07/03 20:38:00 INFO StreamParseJson: no records
17/07/03 20:38:30 INFO StreamParseJson: no records
17/07/03 20:39:00 INFO StreamParseJson: no records
17/07/03 20:39:30 INFO StreamParseJson: no records
17/07/03 20:40:00 INFO StreamParseJson: no records
17/07/03 20:40:30 INFO StreamParseJson: no records
17/07/03 20:41:00 INFO StreamParseJson: no records
17/07/03 20:41:30 INFO StreamParseJson: no records
17/07/03 20:42:00 INFO StreamParseJson: no records
17/07/03 20:42:30 INFO StreamParseJson: no records
17/07/03 20:43:00 INFO StreamParseJson: no records
17/07/03 20:43:30 INFO StreamParseJson: no records
17/07/03 20:44:00 INFO StreamParseJson: no records
17/07/03 20:44:30 INFO StreamParseJson: no records
17/07/03 20:45:00 INFO StreamParseJson: no records
17/07/03 20:45:30 INFO StreamParseJson: no records
17/07/03 20:46:00 INFO StreamParseJson: no records
17/07/03 20:46:30 INFO StreamParseJson: no records
17/07/03 20:47:00 INFO StreamParseJson: no records
17/07/03 20:47:30 INFO StreamParseJson: no records
17/07/03 20:48:00 INFO StreamParseJson: no records
17/07/03 20:48:30 INFO StreamParseJson: no records
17/07/03 20:49:00 INFO StreamParseJson: no records
17/07/03 20:49:30 INFO StreamParseJson: no records
17/07/03 20:50:00 INFO StreamParseJson: no records
17/07/03 20:50:30 INFO StreamParseJson: no records
17/07/03 20:51:00 INFO StreamParseJson: no records
17/07/03 20:51:30 INFO StreamParseJson: no records
17/07/03 20:52:00 INFO StreamParseJson: no records
17/07/03 20:52:30 INFO StreamParseJson: no records
17/07/03 20:53:00 INFO StreamParseJson: no records
17/07/03 20:53:30 INFO StreamParseJson: no records
17/07/03 20:54:00 INFO StreamParseJson: no records
17/07/03 20:54:30 INFO StreamParseJson: no records
17/07/03 20:55:00 INFO StreamParseJson: no records
17/07/03 20:55:30 INFO StreamParseJson: no records
