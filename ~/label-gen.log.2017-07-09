17/07/09 09:00:10 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 09:00:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 09:00:14 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 09:00:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 09:00:18 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 09:00:18 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 09:00:18 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/09 09:00:18 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 09:01:20 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@96ec1ae)
17/07/09 09:01:20 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1499562080133,JobFailed(org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down))
17/07/09 09:04:55 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 09:04:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 09:04:58 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 09:04:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 09:05:01 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 09:05:01 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 09:05:01 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/09 09:05:01 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 09:05:31 INFO StreamParseJson: no records
17/07/09 09:05:31 INFO StreamParseJson: 第1批次完成
17/07/09 09:06:00 INFO StreamParseJson: no records
17/07/09 09:06:00 INFO StreamParseJson: 第2批次完成
17/07/09 09:06:30 INFO StreamParseJson: no records
17/07/09 09:06:30 INFO StreamParseJson: 第3批次完成
17/07/09 09:07:00 INFO StreamParseJson: no records
17/07/09 09:07:00 INFO StreamParseJson: 第4批次完成
17/07/09 09:14:47 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 09:14:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 09:14:50 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 09:14:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 09:14:54 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 09:14:54 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 09:14:54 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/09 09:14:54 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 09:15:01 INFO StreamParseJson: has records
17/07/09 09:15:09 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 09:15:13 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 09:15:16 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 09:15:19 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 09:15:19 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 09:15:23 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 09:15:23 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 09:15:23 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 09:15:24 INFO StreamParseJson: 第1批次完成
17/07/09 09:15:30 INFO StreamParseJson: has records
17/07/09 09:15:31 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 09:15:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 09:15:34 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 09:15:36 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 09:15:36 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 09:15:39 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 09:15:39 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 09:15:39 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 09:15:40 INFO StreamParseJson: 第2批次完成
17/07/09 09:16:00 INFO StreamParseJson: has records
17/07/09 09:16:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 09:16:02 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 09:16:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 09:16:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 09:16:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 09:16:09 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 09:16:09 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 09:16:09 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 09:16:10 INFO StreamParseJson: 第3批次完成
17/07/09 09:16:10 INFO StreamParseJson: 关闭spark streaming,此次共耗时77秒
17/07/09 09:16:10 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event WrappedStreamingListenerEvent(StreamingListenerOutputOperationCompleted(OutputOperationInfo(1499562960000 ms,0,foreachRDD at StreamParseJson.scala:43,org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:625)
com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson.run(StreamParseJson.scala:43)
com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain$.main(StreamParseMain.scala:9)
com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain.main(StreamParseMain.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
com.intellij.rt.execution.application.AppMain.main(AppMain.java:140),Some(1499562960069),Some(1499562970478),None)))
17/07/09 09:16:10 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event WrappedStreamingListenerEvent(StreamingListenerBatchCompleted(BatchInfo(1499562960000 ms,Map(0 -> StreamInputInfo(0,60,Map(offsets -> List(OffsetRange(topic: 'dev-syn-table-reportcontent', partition: 0, range: [1446153 -> 1446183]), OffsetRange(topic: 'dev-syn-table-reportcontent', partition: 1, range: [1446974 -> 1447004])), Description -> topic: dev-syn-table-reportcontent	partition: 0	offsets: 1446153 to 1446183
topic: dev-syn-table-reportcontent	partition: 1	offsets: 1446974 to 1447004))),1499562960063,Some(1499562960070),Some(1499562970478),Map(0 -> OutputOperationInfo(1499562960000 ms,0,foreachRDD at StreamParseJson.scala:43,org.apache.spark.streaming.dstream.DStream.foreachRDD(DStream.scala:625)
com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson.run(StreamParseJson.scala:43)
com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain$.main(StreamParseMain.scala:9)
com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain.main(StreamParseMain.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
com.intellij.rt.execution.application.AppMain.main(AppMain.java:140),Some(1499562960069),Some(1499562970478),None)))))
17/07/09 09:16:10 ERROR JobScheduler: Error in job generator
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.removeRdd(BlockManagerMaster.scala:119)
	at org.apache.spark.SparkContext.unpersistRDD(SparkContext.scala:1704)
	at org.apache.spark.rdd.RDD.unpersist(RDD.scala:216)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:457)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$3.apply(DStream.scala:456)
	at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108)
	at scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:108)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:108)
	at org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:456)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:469)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$clearMetadata$5.apply(DStream.scala:469)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.streaming.dstream.DStream.clearMetadata(DStream.scala:469)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:129)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$clearMetadata$2.apply(DStreamGraph.scala:129)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.streaming.DStreamGraph.clearMetadata(DStreamGraph.scala:129)
	at org.apache.spark.streaming.scheduler.JobGenerator.clearMetadata(JobGenerator.scala:263)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:184)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
17/07/09 09:20:24 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 09:20:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 09:20:27 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 09:20:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 09:20:31 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 09:20:31 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 09:20:31 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/09 09:20:31 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 09:21:01 INFO StreamParseJson: no records
17/07/09 09:21:01 INFO StreamParseJson: 第1批次完成
17/07/09 09:21:30 INFO StreamParseJson: no records
17/07/09 09:21:30 INFO StreamParseJson: 第2批次完成
17/07/09 09:22:00 INFO StreamParseJson: no records
17/07/09 09:22:00 INFO StreamParseJson: 第3批次完成
17/07/09 09:22:02 WARN AbstractLifeCycle: FAILED Spark@4e7a6020{HTTP/1.1}{0.0.0.0:4040}: java.lang.InterruptedException
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
	at org.spark_project.jetty.server.AbstractConnector.doStop(AbstractConnector.java:298)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStop(AbstractNetworkConnector.java:88)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)
	at org.spark_project.jetty.server.Server.doStop(Server.java:445)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)
	at org.apache.spark.ui.ServerInfo.stop(JettyUtils.scala:485)
	at org.apache.spark.ui.WebUI.stop(WebUI.scala:149)
	at org.apache.spark.ui.SparkUI.stop(SparkUI.scala:96)
	at org.apache.spark.SparkContext$$anonfun$stop$2$$anonfun$apply$mcV$sp$2.apply(SparkContext.scala:1809)
	at org.apache.spark.SparkContext$$anonfun$stop$2$$anonfun$apply$mcV$sp$2.apply(SparkContext.scala:1809)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext$$anonfun$stop$2.apply$mcV$sp(SparkContext.scala:1809)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1808)
	at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:705)
	at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:643)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:65)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:44)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 09:22:32 WARN QueuedThreadPool: SparkUI{STOPPING,8<=8<=200,i=0,q=1} Couldn't stop Thread[SparkUI-29-selector-ServerConnectorManager@5e38c04/0,5,main]
17/07/09 09:22:32 WARN AbstractLifeCycle: FAILED org.spark_project.jetty.server.Server@2c0fff64: java.lang.InterruptedException
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
	at org.spark_project.jetty.server.AbstractConnector.doStop(AbstractConnector.java:298)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStop(AbstractNetworkConnector.java:88)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)
	at org.spark_project.jetty.server.Server.doStop(Server.java:445)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)
	at org.apache.spark.ui.ServerInfo.stop(JettyUtils.scala:485)
	at org.apache.spark.ui.WebUI.stop(WebUI.scala:149)
	at org.apache.spark.ui.SparkUI.stop(SparkUI.scala:96)
	at org.apache.spark.SparkContext$$anonfun$stop$2$$anonfun$apply$mcV$sp$2.apply(SparkContext.scala:1809)
	at org.apache.spark.SparkContext$$anonfun$stop$2$$anonfun$apply$mcV$sp$2.apply(SparkContext.scala:1809)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext$$anonfun$stop$2.apply$mcV$sp(SparkContext.scala:1809)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1808)
	at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:705)
	at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:643)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:65)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:44)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 09:22:32 WARN StreamingContext: StreamingContext has already been stopped
17/07/09 10:03:08 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 10:03:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 10:03:11 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 10:03:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 10:03:15 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 10:03:15 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 10:03:15 WARN KafkaUtils: overriding executor group.id to spark-executor-dataetl-labelgen
17/07/09 10:03:15 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 10:05:14 INFO StreamParseJson: has records
17/07/09 10:05:22 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 10:05:26 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 10:05:29 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 10:05:32 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 10:05:32 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 10:05:35 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 10:05:35 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 10:05:36 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 10:05:36 INFO StreamParseJson: 第1批次完成
17/07/09 10:05:37 INFO StreamParseJson: has records
17/07/09 10:05:38 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 10:05:40 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 10:05:40 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 10:05:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 10:05:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 10:05:45 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 10:05:45 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 10:05:45 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 10:05:46 INFO StreamParseJson: 第2批次完成
17/07/09 10:05:47 INFO StreamParseJson: has records
17/07/09 10:05:47 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 10:05:49 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 10:05:49 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 10:05:52 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 10:05:52 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 10:05:54 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 10:05:54 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 10:05:54 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 10:05:55 INFO StreamParseJson: 第3批次完成
17/07/09 10:05:55 INFO StreamParseJson: 共耗时160508秒
17/07/09 10:05:57 WARN AbstractLifeCycle: FAILED Spark@4e7a6020{HTTP/1.1}{0.0.0.0:4040}: java.lang.InterruptedException
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1326)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:277)
	at org.spark_project.jetty.server.AbstractConnector.doStop(AbstractConnector.java:298)
	at org.spark_project.jetty.server.AbstractNetworkConnector.doStop(AbstractNetworkConnector.java:88)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)
	at org.spark_project.jetty.server.Server.doStop(Server.java:445)
	at org.spark_project.jetty.util.component.AbstractLifeCycle.stop(AbstractLifeCycle.java:89)
	at org.apache.spark.ui.ServerInfo.stop(JettyUtils.scala:485)
	at org.apache.spark.ui.WebUI.stop(WebUI.scala:149)
	at org.apache.spark.ui.SparkUI.stop(SparkUI.scala:96)
	at org.apache.spark.SparkContext$$anonfun$stop$2$$anonfun$apply$mcV$sp$2.apply(SparkContext.scala:1809)
	at org.apache.spark.SparkContext$$anonfun$stop$2$$anonfun$apply$mcV$sp$2.apply(SparkContext.scala:1809)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext$$anonfun$stop$2.apply$mcV$sp(SparkContext.scala:1809)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1808)
	at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:705)
	at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:643)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:67)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:45)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13668 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13666 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13664 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13663 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13662 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13671 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13670 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13669 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13667 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:28:40 WARN Sender: Got error produce response with correlation id 13665 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76271 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76270 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76267 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76266 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76263 on topic-partition dev-syn-table-reportcontent-0, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76272 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76269 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76268 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76265 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 10:43:21 WARN Sender: Got error produce response with correlation id 76264 on topic-partition dev-syn-table-reportcontent-1, retrying (0 attempts left). Error: NETWORK_EXCEPTION
17/07/09 13:56:25 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 13:56:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 13:56:29 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 13:56:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 13:56:32 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 13:57:17 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 13:57:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 13:57:20 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 13:57:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 13:57:23 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:00:52 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:00:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:00:55 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:00:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:00:58 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:02:28 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:02:31 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:02:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:02:34 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:04:06 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:04:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:04:09 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:04:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:04:12 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:05:01 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:05:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:05:04 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:05:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:05:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:06:00 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:06:03 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:06:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:06:06 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:06:13 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NumberFormatException: For input string: "dev-syn-table-reportcontent2"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$$anonfun$getKafkaOffset$1.apply(SparkUtils.scala:53)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$$anonfun$getKafkaOffset$1.apply(SparkUtils.scala:53)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:06:13 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NumberFormatException: For input string: "dev-syn-table-reportcontent2"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:580)
	at java.lang.Integer.parseInt(Integer.java:615)
	at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)
	at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$$anonfun$getKafkaOffset$1.apply(SparkUtils.scala:53)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$$anonfun$getKafkaOffset$1.apply(SparkUtils.scala:53)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

17/07/09 14:06:13 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
17/07/09 14:07:43 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:07:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:07:47 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:07:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:07:53 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:08:27 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:08:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:08:30 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:08:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:08:33 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:09:24 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:09:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:09:27 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:09:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:09:30 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:09:37 ERROR StreamingContext: Error starting the context, marking it as stopped
java.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.streaming.DStreamGraph.validate(DStreamGraph.scala:163)
	at org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:512)
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:572)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:571)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain$.main(StreamParseMain.scala:10)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain.main(StreamParseMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
17/07/09 14:11:01 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:11:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:11:04 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:11:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:11:07 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:11:32 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:11:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:11:35 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:11:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:11:38 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:11:45 ERROR StreamingContext: Error starting the context, marking it as stopped
java.lang.IllegalArgumentException: requirement failed: No output operations registered, so nothing to execute
	at scala.Predef$.require(Predef.scala:224)
	at org.apache.spark.streaming.DStreamGraph.validate(DStreamGraph.scala:163)
	at org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:512)
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:572)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:571)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain$.main(StreamParseMain.scala:10)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain.main(StreamParseMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
17/07/09 14:21:44 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:21:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:21:47 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:21:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:21:50 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:21:58 INFO StreamParseJson: Map(dev-syn-table-reportcontent2-0 -> 0, dev-syn-table-reportcontent2-1 -> 0)
17/07/09 14:21:58 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 14:21:58 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 14:21:58 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 14:21:58 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 14:21:58 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 14:22:01 INFO StreamParseJson: has records
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_0
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_2
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_4
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_6
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_8
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_10
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_12
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_14
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_16
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_18
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_20
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_22
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_24
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_26
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_28
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_0
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_2
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_4
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_6
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_8
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_10
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_12
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_14
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_16
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_18
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_20
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_22
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_24
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_26
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_28
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_1
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_3
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_5
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_7
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_9
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_11
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_13
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_15
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_17
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_19
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_21
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_23
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_25
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_27
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_29
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_1
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_3
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_5
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_7
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_9
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_11
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_13
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_15
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_17
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_19
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_21
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_23
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_25
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_27
17/07/09 14:22:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_29
17/07/09 14:22:06 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 14:22:07 ERROR StreamParseJson: 解析报告出错了
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.elasticsearch.spark.rdd.EsSpark$.doSaveToEs(EsSpark.scala:102)
	at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:85)
	at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:80)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.labelsToES(ParseReport.scala:153)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHBase(ParseReport.scala:239)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:69)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:55)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:07 INFO StreamParseJson: 第1批次完成
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@39fe9e61)
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@31a00e24)
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@45d23da0)
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@4066084b)
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@da69bb0)
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(4,1499581328044,JobFailed(org.apache.spark.SparkException: Job 4 cancelled because SparkContext was shut down))
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_17_1 failed due to an exception
17/07/09 14:22:08 ERROR DiskBlockManager: Exception while deleting local spark dir: /tmp/blockmgr-2c57fc46-9cfb-4b90-9279-cd8dabe396b8
java.io.IOException: Failed to delete: /tmp/blockmgr-2c57fc46-9cfb-4b90-9279-cd8dabe396b8
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1010)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:169)
	at org.apache.spark.storage.DiskBlockManager$$anonfun$org$apache$spark$storage$DiskBlockManager$$doStop$1.apply(DiskBlockManager.scala:165)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:165)
	at org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:160)
	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1409)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:89)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1849)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/07/09 14:22:08 WARN BlockManager: Block rdd_17_1 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_17_0 failed due to an exception
17/07/09 14:22:08 WARN BlockManager: Block rdd_17_0 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 192.168.31.11, 43267, None),rdd_17_1,StorageLevel(1 replicas),0,0))
17/07/09 14:22:08 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerBlockUpdated(BlockUpdatedInfo(BlockManagerId(driver, 192.168.31.11, 43267, None),rdd_17_0,StorageLevel(1 replicas),0,0))
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_25_0 failed due to an exception
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_25_1 failed due to an exception
17/07/09 14:22:08 WARN BlockManager: Block rdd_25_1 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 WARN BlockManager: Block rdd_25_0 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 ERROR MapOutputTrackerMaster: Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:100)
	at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:99)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 ERROR TaskContextImpl: Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_7 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:287)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:660)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:123)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:97)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:95)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:95)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 ERROR TaskContextImpl: Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_7 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:287)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:660)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:123)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:97)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:95)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:95)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 ERROR MapOutputTrackerMaster: Error communicating with MapOutputTracker
java.lang.NullPointerException
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:102)
	at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:204)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:144)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:100)
	at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:99)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
	at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.hasNext(InMemoryRelation.scala:132)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:215)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)
	at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_17_0 failed due to an exception
17/07/09 14:22:08 WARN BlockManager: Block rdd_17_0 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_30_0 failed due to an exception
17/07/09 14:22:08 WARN BlockManager: Block rdd_30_0 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 ERROR TaskContextImpl: Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_7 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:287)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:660)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:123)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:97)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:95)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:95)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 ERROR Executor: Exception in task 1.0 in stage 8.0 (TID 9)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:676)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:676)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 ERROR Executor: Exception in task 2.0 in stage 8.0 (TID 10)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:676)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_17_1 failed due to an exception
17/07/09 14:22:08 WARN BlockManager: Block rdd_17_1 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 WARN BlockManager: Putting block rdd_30_1 failed due to an exception
17/07/09 14:22:08 WARN BlockManager: Block rdd_30_1 could not be removed as it was not found on disk or in memory
17/07/09 14:22:08 ERROR TaskContextImpl: Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_7 not found
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at org.apache.spark.storage.BlockInfoManager$$anonfun$1.apply(BlockInfoManager.scala:288)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:287)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:660)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$releaseLock$1.apply(TorrentBroadcast.scala:246)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:123)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:97)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:95)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:95)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:08 ERROR Executor: Exception in task 3.0 in stage 8.0 (TID 11)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:676)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 14:22:42 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 14:22:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 14:22:46 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 14:22:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 14:22:50 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.
17/07/09 14:22:58 INFO StreamParseJson: Map(dev-syn-table-reportcontent2-0 -> 100, dev-syn-table-reportcontent2-1 -> 100)
17/07/09 14:22:58 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 14:22:58 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 14:22:58 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 14:22:58 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 14:22:58 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 14:23:01 INFO StreamParseJson: has records
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_100
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_102
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_104
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_106
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_108
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_110
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_112
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_114
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_116
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_118
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_120
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_122
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_124
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_126
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_128
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_100
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_102
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_104
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_106
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_108
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_110
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_112
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_114
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_116
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_118
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_120
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_122
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_124
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_126
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_128
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_101
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_103
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_105
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_107
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_109
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_111
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_113
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_115
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_117
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_119
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_121
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_123
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_125
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_127
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_1_129
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_101
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_103
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_105
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_107
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_109
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_111
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_113
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_115
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_117
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_119
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_121
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_123
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_125
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_127
17/07/09 14:23:01 INFO StreamParseJson: dev-syn-table-reportcontent2_0_129
17/07/09 15:02:00 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:02:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:02:04 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:02:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:02:14 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 100, dev-syn-table-reportcontent2-1 -> 100)
17/07/09 15:02:14 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:02:14 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:02:14 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:02:14 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:02:14 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:02:31 INFO StreamParseJson: has records
17/07/09 15:02:35 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:02:40 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:02:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:02:46 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:02:46 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:02:49 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:02:49 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:02:49 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:02:50 ERROR Executor: Exception in task 1.0 in stage 33.0 (TID 45)
java.sql.BatchUpdateException: Unknown column 'offset' in 'field list'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'offset' in 'field list'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:02:50 WARN TaskSetManager: Lost task 1.0 in stage 33.0 (TID 45, localhost, executor driver): java.sql.BatchUpdateException: Unknown column 'offset' in 'field list'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'offset' in 'field list'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

17/07/09 15:02:50 ERROR TaskSetManager: Task 1 in stage 33.0 failed 1 times; aborting job
17/07/09 15:02:50 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 33.0 failed 1 times, most recent failure: Lost task 1.0 in stage 33.0 (TID 45, localhost, executor driver): java.sql.BatchUpdateException: Unknown column 'offset' in 'field list'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'offset' in 'field list'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2319)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:77)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:446)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.saveToMySql(SparkUtils.scala:89)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: Unknown column 'offset' in 'field list'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	... 3 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'offset' in 'field list'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:02:50 INFO StreamParseJson: 第1批次完成
17/07/09 15:02:50 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 44)
java.sql.BatchUpdateException: Unknown column 'offset' in 'field list'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'offset' in 'field list'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:03:00 INFO StreamParseJson: has records
17/07/09 15:03:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:03:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:03:04 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:03:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:03:07 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:03:08 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:03:08 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:03:08 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:03:09 ERROR Executor: Exception in task 1.0 in stage 66.0 (TID 90)
java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:03:09 WARN TaskSetManager: Lost task 1.0 in stage 66.0 (TID 90, localhost, executor driver): java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

17/07/09 15:03:09 ERROR Executor: Exception in task 0.0 in stage 66.0 (TID 89)
java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:03:09 ERROR TaskSetManager: Task 1 in stage 66.0 failed 1 times; aborting job
17/07/09 15:03:09 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 66.0 failed 1 times, most recent failure: Lost task 1.0 in stage 66.0 (TID 90, localhost, executor driver): java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2319)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:77)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:446)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.saveToMySql(SparkUtils.scala:89)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	... 3 more
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:03:09 INFO StreamParseJson: 第2批次完成
17/07/09 15:03:30 INFO StreamParseJson: has records
17/07/09 15:03:31 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:03:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:03:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:03:35 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:03:36 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:03:38 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:03:38 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:03:38 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:03:38 ERROR Executor: Exception in task 1.0 in stage 99.0 (TID 135)
java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:03:38 ERROR Executor: Exception in task 0.0 in stage 99.0 (TID 134)
java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:03:38 WARN TaskSetManager: Lost task 1.0 in stage 99.0 (TID 135, localhost, executor driver): java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

17/07/09 15:03:38 ERROR TaskSetManager: Task 1 in stage 99.0 failed 1 times; aborting job
17/07/09 15:03:38 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 99.0 failed 1 times, most recent failure: Lost task 1.0 in stage 99.0 (TID 135, localhost, executor driver): java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2319)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:77)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:446)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.saveToMySql(SparkUtils.scala:89)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	... 3 more
Caused by: java.sql.SQLException: Incorrect integer value: 'labelgen' for column 'partition' at row 1
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1073)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:03:38 INFO StreamParseJson: 第3批次完成
17/07/09 15:04:42 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:04:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:04:47 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:04:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:05:00 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 100, dev-syn-table-reportcontent2-1 -> 100)
17/07/09 15:05:00 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:05:01 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:05:01 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:05:01 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:05:01 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:05:31 INFO StreamParseJson: has records
17/07/09 15:05:35 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:05:39 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:05:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:05:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:05:46 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:05:47 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:05:47 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:05:48 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:05:49 ERROR Executor: Exception in task 1.0 in stage 33.0 (TID 45)
java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:05:49 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 44)
java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:05:49 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 44, localhost, executor driver): java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

17/07/09 15:05:49 ERROR TaskSetManager: Task 0 in stage 33.0 failed 1 times; aborting job
17/07/09 15:05:49 WARN TaskSetManager: Lost task 1.0 in stage 33.0 (TID 45, localhost, executor driver): java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

17/07/09 15:05:49 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 44, localhost, executor driver): java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2319)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:77)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:446)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.saveToMySql(SparkUtils.scala:89)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	... 3 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:05:49 INFO StreamParseJson: 第1批次完成
17/07/09 15:06:00 INFO StreamParseJson: has records
17/07/09 15:06:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:06:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:06:04 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:06:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:06:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:06:08 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:06:08 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:06:08 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:06:09 ERROR Executor: Exception in task 1.0 in stage 66.0 (TID 90)
java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:06:09 WARN TaskSetManager: Lost task 1.0 in stage 66.0 (TID 90, localhost, executor driver): java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

17/07/09 15:06:09 ERROR TaskSetManager: Task 1 in stage 66.0 failed 1 times; aborting job
17/07/09 15:06:09 ERROR StreamParseJson: 解析报告出错了
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 66.0 failed 1 times, most recent failure: Lost task 1.0 in stage 66.0 (TID 90, localhost, executor driver): java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2320)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2788)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2319)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:77)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:446)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.saveToMySql(SparkUtils.scala:89)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	... 3 more
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-0-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:06:09 INFO StreamParseJson: 第2批次完成
17/07/09 15:06:09 ERROR Executor: Exception in task 0.0 in stage 66.0 (TID 89)
java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more
17/07/09 15:06:09 WARN TaskSetManager: Lost task 0.0 in stage 66.0 (TID 89, localhost, executor driver): java.sql.BatchUpdateException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2045)
	at com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1468)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:597)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:670)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:926)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1951)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'dev-syn-table-reportcontent2-1-labelgen' for key 'PRIMARY'
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1039)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2444)
	at com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:1997)
	... 14 more

17/07/09 15:06:30 INFO StreamParseJson: has records
17/07/09 15:06:32 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:06:34 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:06:35 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:08:07 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:08:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:08:10 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:08:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:08:20 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 100, dev-syn-table-reportcontent2-1 -> 100)
17/07/09 15:08:20 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:08:20 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:08:20 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:08:20 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:08:20 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:08:31 INFO StreamParseJson: has records
17/07/09 15:08:34 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:08:39 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:08:42 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:08:44 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:08:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:08:47 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:08:47 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:08:47 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:08:48 INFO StreamParseJson: 第1批次完成
17/07/09 15:09:00 INFO StreamParseJson: has records
17/07/09 15:09:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:09:02 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:09:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:09:05 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:09:05 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:09:07 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:09:07 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:09:07 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:09:08 INFO StreamParseJson: 第2批次完成
17/07/09 15:10:40 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:10:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:10:44 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:10:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:10:54 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 159, dev-syn-table-reportcontent2-1 -> 159)
17/07/09 15:10:54 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:10:54 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:10:54 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:10:54 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:10:54 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:11:01 INFO StreamParseJson: has records
17/07/09 15:11:05 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:11:09 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:11:13 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:11:15 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:11:16 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:11:18 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:11:18 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:11:18 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:11:19 INFO StreamParseJson: 第1批次完成
17/07/09 15:11:30 INFO StreamParseJson: has records
17/07/09 15:11:31 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:11:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:11:34 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:11:36 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:11:37 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:11:38 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:11:38 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:11:38 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:11:39 INFO StreamParseJson: 第2批次完成
17/07/09 15:12:00 INFO StreamParseJson: has records
17/07/09 15:12:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:12:02 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:12:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:12:05 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:12:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:12:07 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:12:07 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:12:07 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:12:08 INFO StreamParseJson: 第3批次完成
17/07/09 15:12:30 INFO StreamParseJson: has records
17/07/09 15:12:31 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:12:32 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:12:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:12:35 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:12:35 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:12:37 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:12:37 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:12:37 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:12:38 INFO StreamParseJson: 第4批次完成
17/07/09 15:24:12 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:24:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:24:16 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:24:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:24:24 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 159, dev-syn-table-reportcontent2-1 -> 159)
17/07/09 15:24:24 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:24:24 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:24:24 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:24:24 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:24:24 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:24:31 INFO StreamParseJson: has records
17/07/09 15:24:34 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:24:39 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:24:41 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:24:44 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:24:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:24:47 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:24:47 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:24:47 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:24:48 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES('dev-syn-table-reportcontent2', 'labelgen', 1, 188) ON' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:24:48 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES('dev-syn-table-reportcontent2', 'labelgen', 0, 188) ON' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:24:48 INFO StreamParseJson: 第1批次完成
17/07/09 15:25:02 INFO StreamParseJson: has records
17/07/09 15:25:04 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:25:06 ERROR StreamParseJson: 解析报告出错了
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:153)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:619)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.elasticsearch.spark.rdd.EsSpark$.doSaveToEs(EsSpark.scala:102)
	at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:85)
	at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:80)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.labelsToES(ParseReport.scala:153)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHBase(ParseReport.scala:239)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:70)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:25:06 INFO StreamParseJson: 第2批次完成
17/07/09 15:25:06 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@5a1f86ce)
17/07/09 15:25:06 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(12,1499585106312,JobFailed(org.apache.spark.SparkException: Job 12 cancelled because SparkContext was shut down))
17/07/09 15:28:02 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:28:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:28:05 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:28:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:28:15 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 159, dev-syn-table-reportcontent2-1 -> 159)
17/07/09 15:28:15 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:28:15 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:28:15 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:28:15 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:28:15 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:28:31 INFO StreamParseJson: has records
17/07/09 15:28:35 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:28:40 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:28:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:28:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:28:46 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:28:48 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:28:48 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:28:48 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:28:49 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 188) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:28:49 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 188) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:28:49 INFO StreamParseJson: 第1批次完成
17/07/09 15:29:00 INFO StreamParseJson: has records
17/07/09 15:29:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:29:01 WARN BlockManager: Block rdd_97_1 already exists on this machine; not re-adding it
17/07/09 15:29:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:29:04 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:29:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:29:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:29:08 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:29:08 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:29:08 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:29:09 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 218) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:29:09 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 218) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:29:09 INFO StreamParseJson: 第2批次完成
17/07/09 15:29:30 INFO StreamParseJson: has records
17/07/09 15:29:31 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:29:32 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:29:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:29:35 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:29:36 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:29:37 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:29:37 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:29:37 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:29:38 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 248) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:29:38 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 248) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:29:38 INFO StreamParseJson: 第3批次完成
17/07/09 15:30:00 INFO StreamParseJson: has records
17/07/09 15:30:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:30:02 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:30:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:30:05 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:30:05 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:30:07 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:30:07 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:30:07 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:30:08 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 278) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:30:08 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 278) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:30:08 INFO StreamParseJson: 第4批次完成
17/07/09 15:30:30 INFO StreamParseJson: has records
17/07/09 15:30:31 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:30:31 WARN BlockManager: Block rdd_340_1 already exists on this machine; not re-adding it
17/07/09 15:30:32 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:30:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:30:35 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:30:35 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:30:37 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:30:37 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:30:37 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:30:38 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 308) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:30:38 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 308) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:30:38 INFO StreamParseJson: 第5批次完成
17/07/09 15:30:53 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:30:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:30:57 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:30:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:31:06 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 159, dev-syn-table-reportcontent2-1 -> 159)
17/07/09 15:31:06 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:31:06 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:31:06 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:31:06 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:31:06 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:31:31 INFO StreamParseJson: has records
17/07/09 15:31:35 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:31:39 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:31:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:31:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:31:46 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:31:48 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:31:48 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:31:48 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:31:49 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 188) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:31:49 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 188) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:31:49 INFO StreamParseJson: 第1批次完成
17/07/09 15:32:00 INFO StreamParseJson: has records
17/07/09 15:32:02 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:32:04 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:32:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:32:08 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:32:08 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:32:11 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:32:11 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:32:11 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:32:12 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 218) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:32:12 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 218) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:32:12 INFO StreamParseJson: 第2批次完成
17/07/09 15:32:30 INFO StreamParseJson: has records
17/07/09 15:32:32 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:32:33 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:32:34 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:32:36 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:32:37 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:32:39 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:32:39 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:32:39 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:32:40 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 1, 248) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:32:40 INFO KafkaTopicOffsetDAO: Mysql insertOrUpdate KafkaTopicOffsetDAO 失败
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'partition, offset) VALUES ('dev-syn-table-reportcontent2', 'labelgen', 0, 248) O' at line 1
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.Util.getInstance(Util.java:386)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4096)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4028)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2490)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2651)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2683)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2144)
	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1379)
	at com.haozhuo.bigdata.dataetl.mysql.KafkaTopicOffsetDAO.insertOrUpdate(KafkaTopicOffsetDAO.java:34)
	at com.haozhuo.bigdata.dataetl.spark.SparkUtils$.commitKafkaOffset(SparkUtils.scala:64)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:72)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:32:40 INFO StreamParseJson: 第3批次完成
17/07/09 15:36:49 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:36:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:36:52 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:36:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:37:02 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 159, dev-syn-table-reportcontent2-1 -> 159)
17/07/09 15:37:02 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:37:02 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:37:02 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:37:02 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:37:02 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:37:31 INFO StreamParseJson: has records
17/07/09 15:37:35 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:37:39 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:37:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:37:46 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:37:47 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:37:49 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:37:49 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:37:49 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:37:50 INFO StreamParseJson: 第1批次完成
17/07/09 15:38:00 INFO StreamParseJson: has records
17/07/09 15:38:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:38:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:38:04 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:38:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:38:06 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:38:08 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:38:08 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:38:08 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:38:09 INFO StreamParseJson: 第2批次完成
17/07/09 15:38:42 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:38:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:38:45 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:38:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:38:53 INFO StreamParseJson: fromOffsets:Map()
17/07/09 15:38:53 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:38:53 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:38:53 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:38:53 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:38:53 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:39:00 INFO StreamParseJson: no records
17/07/09 15:39:00 INFO StreamParseJson: 第1批次完成
17/07/09 15:39:30 INFO StreamParseJson: no records
17/07/09 15:39:30 INFO StreamParseJson: 第2批次完成
17/07/09 15:40:00 INFO StreamParseJson: no records
17/07/09 15:40:00 INFO StreamParseJson: 第3批次完成
17/07/09 15:40:30 INFO StreamParseJson: no records
17/07/09 15:40:30 INFO StreamParseJson: 第4批次完成
17/07/09 15:41:02 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:41:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:41:05 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:41:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:41:16 INFO StreamParseJson: fromOffsets:Map()
17/07/09 15:41:16 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:41:16 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:41:16 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:41:16 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:41:16 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:41:16 ERROR StreamingContext: Error starting the context, marking it as stopped
org.apache.kafka.common.config.ConfigException: Invalid value smallest for configuration auto.offset.reset: String must be one of: latest, earliest, none
	at org.apache.kafka.common.config.ConfigDef$ValidString.ensureValid(ConfigDef.java:870)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:442)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:56)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:63)
	at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:426)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:566)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:549)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:83)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:75)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:243)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach_quick(ParArray.scala:143)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:136)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:577)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:571)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain$.main(StreamParseMain.scala:10)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain.main(StreamParseMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
17/07/09 15:41:51 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:41:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:41:54 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:41:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:42:14 INFO StreamParseJson: fromOffsets:Map()
17/07/09 15:42:14 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:42:14 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:42:14 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:42:14 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:42:14 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:42:31 INFO StreamParseJson: has records
17/07/09 15:42:35 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:42:39 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:42:42 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:42:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:42:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:42:48 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:42:48 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:42:48 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:42:48 INFO StreamParseJson: 第1批次完成
17/07/09 15:43:00 INFO StreamParseJson: has records
17/07/09 15:43:01 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:43:03 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:43:04 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:43:09 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:43:09 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:43:12 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:43:12 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:43:12 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:43:13 INFO StreamParseJson: 第2批次完成
17/07/09 15:44:30 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:44:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:44:33 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:44:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:44:43 INFO StreamParseJson: fromOffsets:Map()
17/07/09 15:44:43 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:44:43 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:44:43 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:44:43 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:44:43 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:44:43 ERROR StreamingContext: Error starting the context, marking it as stopped
org.apache.kafka.common.config.ConfigException: Invalid value lastest for configuration auto.offset.reset: String must be one of: latest, earliest, none
	at org.apache.kafka.common.config.ConfigDef$ValidString.ensureValid(ConfigDef.java:870)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:442)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:56)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:63)
	at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:426)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:566)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:549)
	at org.apache.spark.streaming.kafka010.Subscribe.onStart(ConsumerStrategy.scala:83)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.consumer(DirectKafkaInputDStream.scala:75)
	at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.start(DirectKafkaInputDStream.scala:243)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$start$5.apply(DStreamGraph.scala:49)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach_quick(ParArray.scala:143)
	at scala.collection.parallel.mutable.ParArray$ParArrayIterator.foreach(ParArray.scala:136)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:972)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)
	at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:969)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)
	at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
	at ... run in separate thread using org.apache.spark.util.ThreadUtils ... ()
	at org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:577)
	at org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:571)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain$.main(StreamParseMain.scala:10)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseMain.main(StreamParseMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
17/07/09 15:45:07 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:45:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:45:11 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:45:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:45:36 INFO StreamParseJson: fromOffsets:Map()
17/07/09 15:45:36 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:45:37 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:45:37 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:45:37 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:45:37 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:46:00 INFO StreamParseJson: no records
17/07/09 15:46:00 INFO StreamParseJson: 第1批次完成
17/07/09 15:46:30 INFO StreamParseJson: has records
17/07/09 15:46:35 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:46:40 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:46:43 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:46:45 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:46:46 INFO ParseReport: 将数据保存到HBase的dataetl:report_suggest中
17/07/09 15:46:48 INFO ReportDAO: Mysql insertOrUpdate 60 REPORT:
17/07/09 15:46:48 INFO ParseReport: 将60条report保存到Mysql中完成
17/07/09 15:46:48 INFO ParseReport: 将数据保存到HBase的dataetl:reports中
17/07/09 15:46:49 INFO StreamParseJson: 第2批次完成
17/07/09 15:48:27 INFO ScalaUtils$: 加载本地目录下的文件:/data/work/luciuschina/data-etl/label-gen/target/classes/config.properties
17/07/09 15:48:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/07/09 15:48:31 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 192.168.31.11 instead (on interface eno16777736)
17/07/09 15:48:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
17/07/09 15:48:40 INFO StreamParseJson: fromOffsets:Map(dev-syn-table-reportcontent2-0 -> 1213, dev-syn-table-reportcontent2-1 -> 1216)
17/07/09 15:48:40 INFO StreamParseJson: app.stream.batch.num:3
17/07/09 15:48:40 WARN KafkaUtils: overriding enable.auto.commit to false for executor
17/07/09 15:48:40 WARN KafkaUtils: overriding auto.offset.reset to none for executor
17/07/09 15:48:40 WARN KafkaUtils: overriding executor group.id to spark-executor-labelgen
17/07/09 15:48:40 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135
17/07/09 15:49:01 INFO StreamParseJson: has records
17/07/09 15:49:05 INFO ParseReport: 将解析完的报告存入ES中
17/07/09 15:49:10 INFO ParseReport: 将数据保存到HBase的dataetl:report_label中
17/07/09 15:49:14 INFO ParseReport: 将数据保存到HBase的dataetl:report_index中
17/07/09 15:49:16 INFO ParseReport: 将数据保存到HBase的dataetl:report_summary中
17/07/09 15:49:16 WARN ZKUtil: hconnection-0x736065250x0, quorum=192.168.1.150:2181, baseZNode=/hbase Unable to set watcher on znode (/hbase/hbaseid)
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)
	at org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1040)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:220)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.checkExists(ZKUtil.java:419)
	at org.apache.hadoop.hbase.zookeeper.ZKClusterId.readClusterIdZNode(ZKClusterId.java:65)
	at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getClusterId(ZooKeeperRegistry.java:105)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.retrieveClusterId(ConnectionManager.java:905)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.<init>(ConnectionManager.java:648)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:238)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:218)
	at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:119)
	at com.haozhuo.bigdata.dataetl.hbase.HBaseClient.putInto(HBaseClient.scala:38)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.summaryToHBase(ParseReport.scala:205)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHBase(ParseReport.scala:242)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:70)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:49:16 ERROR AsyncProcess: Failed to get region location 
java.io.InterruptedIOException: Giving up trying to location region in meta: thread is interrupted.
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1363)
	at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1181)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:410)
	at org.apache.hadoop.hbase.client.AsyncProcess.submit(AsyncProcess.java:359)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:238)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:190)
	at com.haozhuo.bigdata.dataetl.hbase.HBaseClient.putInto(HBaseClient.scala:51)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.summaryToHBase(ParseReport.scala:205)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHBase(ParseReport.scala:242)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:70)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
17/07/09 15:49:17 ERROR HBaseClient: 关闭HBase客户端连接出现错误
org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException: Failed 1 action: InterruptedIOException: 1 time, 
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.makeException(AsyncProcess.java:258)
	at org.apache.hadoop.hbase.client.AsyncProcess$BatchErrors.access$2000(AsyncProcess.java:238)
	at org.apache.hadoop.hbase.client.AsyncProcess.waitForAllPreviousOpsAndReset(AsyncProcess.java:1817)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.backgroundFlushCommits(BufferedMutatorImpl.java:240)
	at org.apache.hadoop.hbase.client.BufferedMutatorImpl.flush(BufferedMutatorImpl.java:190)
	at com.haozhuo.bigdata.dataetl.hbase.HBaseClient.putInto(HBaseClient.scala:51)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.summaryToHBase(ParseReport.scala:205)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.ParseReport.saveToEsHBase(ParseReport.scala:242)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:70)
	at com.haozhuo.bigdata.dataetl.labelgen.spark.stream.StreamParseJson$$anonfun$run$2.apply(StreamParseJson.scala:56)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:256)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:255)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
